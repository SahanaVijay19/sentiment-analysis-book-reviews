{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Define and Solve an ML Problem of Your Choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab assignment, you will follow the machine learning life cycle and implement a model to solve a machine learning problem of your choosing. You will select a data set and choose a predictive problem that the data set supports.  You will then inspect the data with your problem in mind and begin to formulate a  project plan. You will then implement the machine learning project plan. \n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "1. Build Your DataFrame\n",
    "2. Define Your ML Problem\n",
    "3. Perform exploratory data analysis to understand your data.\n",
    "4. Define Your Project Plan\n",
    "5. Implement Your Project Plan:\n",
    "    * Prepare your data for your model.\n",
    "    * Fit your model to the training data and evaluate your model.\n",
    "    * Improve your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Build Your DataFrame\n",
    "\n",
    "You will have the option to choose one of four data sets that you have worked with in this program:\n",
    "\n",
    "* The \"census\" data set that contains Census information from 1994: `censusData.csv`\n",
    "* Airbnb NYC \"listings\" data set: `airbnbListingsData.csv`\n",
    "* World Happiness Report (WHR) data set: `WHR2018Chapter2OnlineData.csv`\n",
    "* Book Review data set: `bookReviewsData.csv`\n",
    "\n",
    "Note that these are variations of the data sets that you have worked with in this program. For example, some do not include some of the preprocessing necessary for specific models. \n",
    "\n",
    "#### Load a Data Set and Save it as a Pandas DataFrame\n",
    "\n",
    "The code cell below contains filenames (path + filename) for each of the four data sets available to you.\n",
    "\n",
    "<b>Task:</b> In the code cell below, use the same method you have been using to load the data using `pd.read_csv()` and save it to DataFrame `df`. \n",
    "\n",
    "You can load each file as a new DataFrame to inspect the data before choosing your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was perhaps the best of Johannes Steinhof...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This very fascinating book is a story written ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The four tales in this collection are beautifu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The book contained more profanity than I expec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We have now entered a second time of deep conc...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I don't know why it won the National Book Awar...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The daughter of a prominent Boston doctor is d...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I was very disapointed in the book.Basicly the...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I think in retrospect I wasted my time on this...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I have a hard time understanding what it is th...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Positive Review\n",
       "0  This was perhaps the best of Johannes Steinhof...             True\n",
       "1  This very fascinating book is a story written ...             True\n",
       "2  The four tales in this collection are beautifu...             True\n",
       "3  The book contained more profanity than I expec...            False\n",
       "4  We have now entered a second time of deep conc...             True\n",
       "5  I don't know why it won the National Book Awar...            False\n",
       "6  The daughter of a prominent Boston doctor is d...            False\n",
       "7  I was very disapointed in the book.Basicly the...            False\n",
       "8  I think in retrospect I wasted my time on this...            False\n",
       "9  I have a hard time understanding what it is th...            False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File names of the four data sets\n",
    "adultDataSet_filename = os.path.join(os.getcwd(), \"data\", \"censusData.csv\")\n",
    "airbnbDataSet_filename = os.path.join(os.getcwd(), \"data\", \"airbnbListingsData.csv\")\n",
    "WHRDataSet_filename = os.path.join(os.getcwd(), \"data\", \"WHR2018Chapter2OnlineData.csv\")\n",
    "bookReviewDataSet_filename = os.path.join(os.getcwd(), \"data\", \"bookReviewsData.csv\")\n",
    "\n",
    "# I am choosing the bookReviewsData.csv dataset for my project.\n",
    "\n",
    "df = pd.read_csv(bookReviewDataSet_filename )\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Define Your ML Problem\n",
    "\n",
    "Next you will formulate your ML Problem. In the markdown cell below, answer the following questions:\n",
    "\n",
    "1. List the data set you have chosen.\n",
    "2. What will you be predicting? What is the label?\n",
    "3. Is this a supervised or unsupervised learning problem? Is this a clustering, classification or regression problem? Is it a binary classificaiton or multi-class classifiction problem?\n",
    "4. What are your features? (note: this list may change after your explore your data)\n",
    "5. Explain why this is an important problem. In other words, how would a company create value with a model that predicts this label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The data set I have chosen is \"bookReviewsData.csv\"\n",
    "2. I will be predicting whether a book review is positive or not. My label is 'Positive Review', which will be either true or false, and my only feature for this problem is the 'Review' column.\n",
    "3. This is a supervised learning problem because for each data point, the 'Review' feature is associated with a 'Positive Review' label. In other words, during the training phase, the ML model is supplied a label along with its features rather than having to draw patterns and groupings by itself. Furthermore, this is a binary classification problem because the label has only one of two outcomes or classes, true or false.\n",
    "4. As mentioned previously, my feature is \"Review\". This is the only feature column for my ML problem.\n",
    "5. This model could be especially useful for websites like Good Reads or publishing companies. Good Reads could used this sentiment analysis model to classify a review left under a book as positive or negative. Furthermore, publishing houses like disney hyperion or wharton could use this model to analyze reviews left for their authors' books and classify them into positive or negative reviews. This model would be a streamlined method for a publishing house to decide the public opinion of their authors' books rather than having employees manually reading through thousands of reviews. Knowing how many positive and negative reviews a given book has can inform the type of books the publishing house publishes in the future, hence can indirectly help boost their profits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understand Your Data\n",
    "\n",
    "The next step is to perform exploratory data analysis. Inspect and analyze your data set with your machine learning problem in mind. Consider the following as you inspect your data:\n",
    "\n",
    "1. What data preparation techniques would you like to use? These data preparation techniques may include:\n",
    "\n",
    "    * addressing missingness, such as replacing missing values with means\n",
    "    * finding and replacing outliers\n",
    "    * renaming features and labels\n",
    "    * finding and replacing outliers\n",
    "    * performing feature engineering techniques such as one-hot encoding on categorical features\n",
    "    * selecting appropriate features and removing irrelevant features\n",
    "    * performing specific data cleaning and preprocessing techniques for an NLP problem\n",
    "    * addressing class imbalance in your data sample to promote fair AI\n",
    "    \n",
    "\n",
    "2. What machine learning model (or models) you would like to use that is suitable for your predictive problem and data?\n",
    "    * Are there other data preparation techniques that you will need to apply to build a balanced modeling data set for your problem and model? For example, will you need to scale your data?\n",
    " \n",
    " \n",
    "3. How will you evaluate and improve the model's performance?\n",
    "    * Are there specific evaluation metrics and methods that are appropriate for your model?\n",
    "    \n",
    "\n",
    "Think of the different techniques you have used to inspect and analyze your data in this course. These include using Pandas to apply data filters, using the Pandas `describe()` method to get insight into key statistics for each column, using the Pandas `dtypes` property to inspect the data type of each column, and using Matplotlib and Seaborn to detect outliers and visualize relationships between features and labels. If you are working on a classification problem, use techniques you have learned to determine if there is class imbalance.\n",
    "\n",
    "<b>Task</b>: Use the techniques you have learned in this course to inspect and analyze your data. You can import additional packages that you have used in this course that you will need to perform this task.\n",
    "\n",
    "<b>Note</b>: You can add code cells if needed by going to the <b>Insert</b> menu and clicking on <b>Insert Cell Below</b> in the drop-drown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Positive Review\n",
      "0  This was perhaps the best of Johannes Steinhof...             True\n",
      "1  This very fascinating book is a story written ...             True\n",
      "2  The four tales in this collection are beautifu...             True\n",
      "3  The book contained more profanity than I expec...            False\n",
      "4  We have now entered a second time of deep conc...             True\n",
      "5  I don't know why it won the National Book Awar...            False\n",
      "6  The daughter of a prominent Boston doctor is d...            False\n",
      "7  I was very disapointed in the book.Basicly the...            False\n",
      "8  I think in retrospect I wasted my time on this...            False\n",
      "9  I have a hard time understanding what it is th...            False\n",
      "(1973, 2)\n",
      "Index(['Review', 'Positive Review'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Routine Data Frame inspections:\n",
    "\n",
    "print(df.head(10))\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review             0\n",
       "Positive Review    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am checking if any values in each column are NaN or null\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Understanding my data:\n",
    "\n",
    "1. Admittedly, in the data understanding stage, since I don't have any numerical data, there are no trends and patterns I have to infer and visualize. Since this is an NLP problem and I have only one feature column, most of the data preperation techniques listed above like df.describe() and dtypes are not applicable to my problem becuase they pertain to categorical or numerical data. Instead, I will do data cleaning tasks like removing whitespaces, lowercasing all words, removing stopwords and perform pre-processing steps like tokenization or lemmatization before I vectorize the text using TF-IDF.\n",
    "2. I will be using a feedforward neural network for my problem and using to predict the probability of the Positive Review label being true. Hence, this is a supervised binary classification problem as specified previously. I will vectorize my text using TF-IDF Vectorization. Beyond this, I am not sure I will need to scale or further process my data before modelling.\n",
    "3. While fitting the model, I will supply parameters such that validation and training loss and accuracy will be displayed. I will use loss and accuracy metrics to evaluate each model configuration to choose the best performing models. I will also evaluate the model's performance on the test set using training loss and accuracy, via the evaluate() method. To keep it simple, I will test 3 different model configurations and choose the best performing model to make final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Define Your Project Plan\n",
    "\n",
    "Now that you understand your data, in the markdown cell below, define your plan to implement the remaining phases of the machine learning life cycle (data preparation, modeling, evaluation) to solve your ML problem. Answer the following questions:\n",
    "\n",
    "* Do you have a new feature list? If so, what are the features that you chose to keep and remove after inspecting the data? \n",
    "* Explain different data preparation techniques that you will use to prepare your data for modeling.\n",
    "* What is your model (or models)?\n",
    "* Describe your plan to train your model, analyze its performance and then improve the model. That is, describe your model building, validation and selection plan to produce a model that generalizes well to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I do not have a new feature list. The only feature remains 'Review'.\n",
    "2. I will remove stop words, lowercase all words, and remove whitespaces. Then, I will perform tokenization and perform TF-IDF vectorization of my text. I plan to utilize the parameters already provided in the TF_IDF object to do the pre-processing and data preparation to make the process easier and more streamlined.\n",
    "3. My model is a feedforward neural network, the simplest neural network, suitable for small data sets. I have chosen this model for ease of implementation and interpretability. A larger corpus and more examples would have required a more complex model.\n",
    "4. I will vectorize my text data. I will then create an input layer, 3 hidden layers, and an output layer for each model configuration. I will define my loss function and optimizer and use those to do a compile of the model. To analyze each model, I will use training loss and accuracy as well as test loss and accuracy. I will also plot the model's performance over time. I am unsure if GridSearchCV is compatible with neural networks so I will manually test a few hyperparameter configurations. I will change the num epochs and add dropout regularization between hidden layers to create different model configurations. I will select the model with the highest accuracy and lowest loss. Finally, I will make predictions using this final optimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Implement Your Project Plan\n",
    "\n",
    "<b>Task:</b> In the code cell below, import additional packages that you have used in this course that you will need to implement your project plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Use the rest of this notebook to carry out your project plan. \n",
    "\n",
    "You will:\n",
    "\n",
    "1. Prepare your data for your model.\n",
    "2. Fit your model to the training data and evaluate your model.\n",
    "3. Improve your model's performance by performing model selection and/or feature selection techniques to find best model for your problem.\n",
    "\n",
    "Add code cells below and populate the notebook with commentary, code, analyses, results, and figures as you see fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdecode_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpreprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'(?u)\\\\b\\\\w\\\\w+\\\\b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'numpy.float64'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msmooth_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
       "\n",
       "Equivalent to :class:`CountVectorizer` followed by\n",
       ":class:`TfidfTransformer`.\n",
       "\n",
       "For an example of usage, see\n",
       ":ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`.\n",
       "\n",
       "For an efficiency comparison of the different feature extractors, see\n",
       ":ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n",
       "\n",
       "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "input : {'filename', 'file', 'content'}, default='content'\n",
       "    - If `'filename'`, the sequence passed as an argument to fit is\n",
       "      expected to be a list of filenames that need reading to fetch\n",
       "      the raw content to analyze.\n",
       "\n",
       "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
       "      object) that is called to fetch the bytes in memory.\n",
       "\n",
       "    - If `'content'`, the input is expected to be a sequence of items that\n",
       "      can be of type string or byte.\n",
       "\n",
       "encoding : str, default='utf-8'\n",
       "    If bytes or files are given to analyze, this encoding is used to\n",
       "    decode.\n",
       "\n",
       "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
       "    Instruction on what to do if a byte sequence is given to analyze that\n",
       "    contains characters not of the given `encoding`. By default, it is\n",
       "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
       "    values are 'ignore' and 'replace'.\n",
       "\n",
       "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
       "    Remove accents and perform other character normalization\n",
       "    during the preprocessing step.\n",
       "    'ascii' is a fast method that only works on characters that have\n",
       "    a direct ASCII mapping.\n",
       "    'unicode' is a slightly slower method that works on any characters.\n",
       "    None (default) means no character normalization is performed.\n",
       "\n",
       "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
       "    :func:`unicodedata.normalize`.\n",
       "\n",
       "lowercase : bool, default=True\n",
       "    Convert all characters to lowercase before tokenizing.\n",
       "\n",
       "preprocessor : callable, default=None\n",
       "    Override the preprocessing (string transformation) stage while\n",
       "    preserving the tokenizing and n-grams generation steps.\n",
       "    Only applies if ``analyzer`` is not callable.\n",
       "\n",
       "tokenizer : callable, default=None\n",
       "    Override the string tokenization step while preserving the\n",
       "    preprocessing and n-grams generation steps.\n",
       "    Only applies if ``analyzer == 'word'``.\n",
       "\n",
       "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
       "    Whether the feature should be made of word or character n-grams.\n",
       "    Option 'char_wb' creates character n-grams only from text inside\n",
       "    word boundaries; n-grams at the edges of words are padded with space.\n",
       "\n",
       "    If a callable is passed it is used to extract the sequence of features\n",
       "    out of the raw, unprocessed input.\n",
       "\n",
       "    .. versionchanged:: 0.21\n",
       "        Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n",
       "        is first read from the file and then passed to the given callable\n",
       "        analyzer.\n",
       "\n",
       "stop_words : {'english'}, list, default=None\n",
       "    If a string, it is passed to _check_stop_list and the appropriate stop\n",
       "    list is returned. 'english' is currently the only supported string\n",
       "    value.\n",
       "    There are several known issues with 'english' and you should\n",
       "    consider an alternative (see :ref:`stop_words`).\n",
       "\n",
       "    If a list, that list is assumed to contain stop words, all of which\n",
       "    will be removed from the resulting tokens.\n",
       "    Only applies if ``analyzer == 'word'``.\n",
       "\n",
       "    If None, no stop words will be used. In this case, setting `max_df`\n",
       "    to a higher value, such as in the range (0.7, 1.0), can automatically detect\n",
       "    and filter stop words based on intra corpus document frequency of terms.\n",
       "\n",
       "token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
       "    Regular expression denoting what constitutes a \"token\", only used\n",
       "    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
       "    or more alphanumeric characters (punctuation is completely ignored\n",
       "    and always treated as a token separator).\n",
       "\n",
       "    If there is a capturing group in token_pattern then the\n",
       "    captured group content, not the entire match, becomes the token.\n",
       "    At most one capturing group is permitted.\n",
       "\n",
       "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
       "    The lower and upper boundary of the range of n-values for different\n",
       "    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
       "    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
       "    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
       "    only bigrams.\n",
       "    Only applies if ``analyzer`` is not callable.\n",
       "\n",
       "max_df : float or int, default=1.0\n",
       "    When building the vocabulary ignore terms that have a document\n",
       "    frequency strictly higher than the given threshold (corpus-specific\n",
       "    stop words).\n",
       "    If float in range [0.0, 1.0], the parameter represents a proportion of\n",
       "    documents, integer absolute counts.\n",
       "    This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "min_df : float or int, default=1\n",
       "    When building the vocabulary ignore terms that have a document\n",
       "    frequency strictly lower than the given threshold. This value is also\n",
       "    called cut-off in the literature.\n",
       "    If float in range of [0.0, 1.0], the parameter represents a proportion\n",
       "    of documents, integer absolute counts.\n",
       "    This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "max_features : int, default=None\n",
       "    If not None, build a vocabulary that only consider the top\n",
       "    `max_features` ordered by term frequency across the corpus.\n",
       "    Otherwise, all features are used.\n",
       "\n",
       "    This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "vocabulary : Mapping or iterable, default=None\n",
       "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
       "    indices in the feature matrix, or an iterable over terms. If not\n",
       "    given, a vocabulary is determined from the input documents.\n",
       "\n",
       "binary : bool, default=False\n",
       "    If True, all non-zero term counts are set to 1. This does not mean\n",
       "    outputs will have only 0/1 values, only that the tf term in tf-idf\n",
       "    is binary. (Set `binary` to True, `use_idf` to False and\n",
       "    `norm` to None to get 0/1 outputs).\n",
       "\n",
       "dtype : dtype, default=float64\n",
       "    Type of the matrix returned by fit_transform() or transform().\n",
       "\n",
       "norm : {'l1', 'l2'} or None, default='l2'\n",
       "    Each output row will have unit norm, either:\n",
       "\n",
       "    - 'l2': Sum of squares of vector elements is 1. The cosine\n",
       "      similarity between two vectors is their dot product when l2 norm has\n",
       "      been applied.\n",
       "    - 'l1': Sum of absolute values of vector elements is 1.\n",
       "      See :func:`~sklearn.preprocessing.normalize`.\n",
       "    - None: No normalization.\n",
       "\n",
       "use_idf : bool, default=True\n",
       "    Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n",
       "\n",
       "smooth_idf : bool, default=True\n",
       "    Smooth idf weights by adding one to document frequencies, as if an\n",
       "    extra document was seen containing every term in the collection\n",
       "    exactly once. Prevents zero divisions.\n",
       "\n",
       "sublinear_tf : bool, default=False\n",
       "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "vocabulary_ : dict\n",
       "    A mapping of terms to feature indices.\n",
       "\n",
       "fixed_vocabulary_ : bool\n",
       "    True if a fixed vocabulary of term to indices mapping\n",
       "    is provided by the user.\n",
       "\n",
       "idf_ : array of shape (n_features,)\n",
       "    The inverse document frequency (IDF) vector; only defined\n",
       "    if ``use_idf`` is True.\n",
       "\n",
       "stop_words_ : set\n",
       "    Terms that were ignored because they either:\n",
       "\n",
       "      - occurred in too many documents (`max_df`)\n",
       "      - occurred in too few documents (`min_df`)\n",
       "      - were cut off by feature selection (`max_features`).\n",
       "\n",
       "    This is only available if no vocabulary was given.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
       "\n",
       "TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
       "    matrix of counts.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The ``stop_words_`` attribute can get large and increase the model size\n",
       "when pickling. This attribute is provided only for introspection and can\n",
       "be safely removed using delattr or set to None before pickling.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
       ">>> corpus = [\n",
       "...     'This is the first document.',\n",
       "...     'This document is the second document.',\n",
       "...     'And this is the third one.',\n",
       "...     'Is this the first document?',\n",
       "... ]\n",
       ">>> vectorizer = TfidfVectorizer()\n",
       ">>> X = vectorizer.fit_transform(corpus)\n",
       ">>> vectorizer.get_feature_names_out()\n",
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], ...)\n",
       ">>> print(X.shape)\n",
       "(4, 9)\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/3.9.19/lib/python3.9/site-packages/sklearn/feature_extraction/text.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating labelled examples and train, test splits\n",
    "\n",
    "y = df['Positive Review'] \n",
    "X = df['Review']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1234)\n",
    "\n",
    "# Data Preparation and TF-IDF Vectorization of text data\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True,stop_words='english')#Tokenization is done by whitespace automatically.\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Creating a vocabulary size\n",
    "\n",
    "vocabulary_size = len(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                1168768   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,171,393\n",
      "Trainable params: 1,171,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 03:38:54.221384: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-07-30 03:38:54.221413: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-07-30 03:38:54.221487: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (i-015ce9d825cc1dcbc): /proc/driver/nvidia/version does not exist\n",
      "2025-07-30 03:38:54.221728: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Creating the Neural Network Configuration 1\n",
    "\n",
    "nn_model = keras.Sequential()\n",
    "\n",
    "input_layer = keras.layers.InputLayer(input_shape=vocabulary_size)\n",
    "nn_model.add(input_layer)\n",
    "\n",
    "hidden_1 = keras.layers.Dense(units=64,activation='relu')\n",
    "nn_model.add(hidden_1)\n",
    "\n",
    "hidden_2 = keras.layers.Dense(units=32,activation='relu')\n",
    "nn_model.add(hidden_2)\n",
    "\n",
    "hidden_3 = keras.layers.Dense(units=16,activation='relu')\n",
    "nn_model.add(hidden_3)\n",
    "\n",
    "output_layer = keras.layers.Dense(units=1,activation='sigmoid')\n",
    "nn_model.add(output_layer)\n",
    "\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Optimization function, Loss function, and compiling model configuration 1\n",
    "\n",
    "sgd_optimizer = keras.optimizers.SGD(learning_rate=0.1)\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "nn_model.compile(optimizer=sgd_optimizer,loss=loss_fn,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom class from Unit 8 assignment to display loss and accuracy and other custom data while the model is running.\n",
    "class ProgBarLoggerNEpochs(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, num_epochs: int, every_n: int = 50):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.every_n = every_n\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.every_n == 0:\n",
    "            s = 'Epoch [{}/ {}]'.format(epoch + 1, self.num_epochs)\n",
    "            logs_s = ['{}: {:.4f}'.format(k.capitalize(), v)\n",
    "                      for k, v in logs.items()]\n",
    "            s_list = [s] + logs_s\n",
    "            print(', '.join(s_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 03:42:00.788627: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2025-07-30 03:42:00.792655: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2649995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 55], Loss: 0.6917, Accuracy: 0.5114, Val_loss: 0.6919, Val_accuracy: 0.5101\n",
      "Epoch [10/ 55], Loss: 0.6831, Accuracy: 0.5985, Val_loss: 0.6858, Val_accuracy: 0.7297\n",
      "Epoch [15/ 55], Loss: 0.6218, Accuracy: 0.7523, Val_loss: 0.6429, Val_accuracy: 0.7230\n",
      "Epoch [20/ 55], Loss: 0.5068, Accuracy: 0.7667, Val_loss: 0.5430, Val_accuracy: 0.7703\n",
      "Epoch [25/ 55], Loss: 0.5949, Accuracy: 0.6839, Val_loss: 0.6648, Val_accuracy: 0.5912\n",
      "Epoch [30/ 55], Loss: 0.3992, Accuracy: 0.8301, Val_loss: 0.4470, Val_accuracy: 0.7838\n",
      "Epoch [35/ 55], Loss: 0.0301, Accuracy: 1.0000, Val_loss: 0.4584, Val_accuracy: 0.7905\n",
      "Epoch [40/ 55], Loss: 0.0076, Accuracy: 1.0000, Val_loss: 0.5022, Val_accuracy: 0.7905\n",
      "Epoch [45/ 55], Loss: 0.0037, Accuracy: 1.0000, Val_loss: 0.5377, Val_accuracy: 0.7770\n",
      "Epoch [50/ 55], Loss: 0.0023, Accuracy: 1.0000, Val_loss: 0.5713, Val_accuracy: 0.7804\n",
      "Epoch [55/ 55], Loss: 0.0016, Accuracy: 1.0000, Val_loss: 0.5752, Val_accuracy: 0.7770\n",
      "Elapsed time: 5.63s\n"
     ]
    }
   ],
   "source": [
    "# Fitting Model Configuration 1\n",
    "\n",
    "t0 = time.time() # start time\n",
    "\n",
    "num_epochs = 55 # epochs\n",
    "\n",
    "history = nn_model.fit(X_train_tfidf.toarray(),y_train,epochs=num_epochs,verbose=0,validation_split=0.2,callbacks=[ProgBarLoggerNEpochs(num_epochs, every_n=5)])\n",
    "\n",
    "t1 = time.time() # stop time\n",
    "\n",
    "print('Elapsed time: %.2fs' % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5514 - accuracy: 0.8057\n",
      "Loss:  0.5513968467712402 Accuracy:  0.8056679964065552\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Model Configuration 1's performance on test data\n",
    "\n",
    "loss, accuracy = nn_model.evaluate(X_test_tfidf.toarray(),y_test)\n",
    "print('Loss: ', str(loss) , 'Accuracy: ', str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                1168768   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,171,393\n",
      "Trainable params: 1,171,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating the Neural Network Configuration 2: Dropout Regularization after every hidden layer\n",
    "\n",
    "nn2_model = keras.Sequential()\n",
    "\n",
    "input_layer = keras.layers.InputLayer(input_shape=vocabulary_size)\n",
    "nn2_model.add(input_layer)\n",
    "\n",
    "hidden_1 = keras.layers.Dense(units=64,activation='relu')\n",
    "nn2_model.add(hidden_1)\n",
    "nn2_model.add(keras.layers.Dropout(.25))\n",
    "\n",
    "hidden_2 = keras.layers.Dense(units=32,activation='relu')\n",
    "nn2_model.add(hidden_2)\n",
    "nn2_model.add(keras.layers.Dropout(.25))\n",
    "\n",
    "hidden_3 = keras.layers.Dense(units=16,activation='relu')\n",
    "nn2_model.add(hidden_3)\n",
    "nn2_model.add(keras.layers.Dropout(.25))\n",
    "\n",
    "output_layer = keras.layers.Dense(units=1,activation='sigmoid')\n",
    "nn2_model.add(output_layer)\n",
    "\n",
    "nn2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 55], Loss: 0.6924, Accuracy: 0.5080, Val_loss: 0.6924, Val_accuracy: 0.5101\n",
      "Epoch [10/ 55], Loss: 0.6876, Accuracy: 0.5630, Val_loss: 0.6895, Val_accuracy: 0.5101\n",
      "Epoch [15/ 55], Loss: 0.6540, Accuracy: 0.6847, Val_loss: 0.6668, Val_accuracy: 0.7669\n",
      "Epoch [20/ 55], Loss: 0.5460, Accuracy: 0.7473, Val_loss: 0.7031, Val_accuracy: 0.5270\n",
      "Epoch [25/ 55], Loss: 0.4426, Accuracy: 0.8132, Val_loss: 0.4739, Val_accuracy: 0.7973\n",
      "Epoch [30/ 55], Loss: 0.3285, Accuracy: 0.8783, Val_loss: 0.4601, Val_accuracy: 0.7838\n",
      "Epoch [35/ 55], Loss: 0.2550, Accuracy: 0.9341, Val_loss: 0.4528, Val_accuracy: 0.7770\n",
      "Epoch [40/ 55], Loss: 0.0658, Accuracy: 0.9941, Val_loss: 0.4963, Val_accuracy: 0.7939\n",
      "Epoch [45/ 55], Loss: 0.0292, Accuracy: 0.9949, Val_loss: 0.5975, Val_accuracy: 0.7804\n",
      "Epoch [50/ 55], Loss: 0.0155, Accuracy: 0.9983, Val_loss: 0.6912, Val_accuracy: 0.7905\n",
      "Epoch [55/ 55], Loss: 0.4348, Accuracy: 0.8284, Val_loss: 0.5935, Val_accuracy: 0.6216\n",
      "Elapsed time: 5.86s\n"
     ]
    }
   ],
   "source": [
    "# Compiling and fitting model configuration 2\n",
    "\n",
    "nn2_model.compile(optimizer=sgd_optimizer,loss=loss_fn,metrics=['accuracy'])\n",
    "t0 = time.time() # start time\n",
    "num_epochs = 55 # epochs\n",
    "history = nn2_model.fit(X_train_tfidf.toarray(),y_train,epochs=num_epochs,verbose=0,validation_split=0.2,callbacks=[ProgBarLoggerNEpochs(num_epochs, every_n=5)])\n",
    "t1 = time.time() # stop time\n",
    "print('Elapsed time: %.2fs' % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5565 - accuracy: 0.6640\n",
      "Loss:  0.5565044283866882 Accuracy:  0.6639676094055176\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Model Configuration 2's performance on test data\n",
    "\n",
    "loss, accuracy = nn2_model.evaluate(X_test_tfidf.toarray(),y_test)\n",
    "print('Loss: ', str(loss) , 'Accuracy: ', str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 64)                1168768   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,171,393\n",
      "Trainable params: 1,171,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating the Neural Network Configuration 3: Changing the num epochs to 45.\n",
    "\n",
    "nn3_model = keras.Sequential()\n",
    "\n",
    "input_layer = keras.layers.InputLayer(input_shape=vocabulary_size)\n",
    "nn3_model.add(input_layer)\n",
    "\n",
    "hidden_1 = keras.layers.Dense(units=64,activation='relu')\n",
    "nn3_model.add(hidden_1)\n",
    "\n",
    "hidden_2 = keras.layers.Dense(units=32,activation='relu')\n",
    "nn3_model.add(hidden_2)\n",
    "\n",
    "hidden_3 = keras.layers.Dense(units=16,activation='relu')\n",
    "nn3_model.add(hidden_3)\n",
    "\n",
    "output_layer = keras.layers.Dense(units=1,activation='sigmoid')\n",
    "nn3_model.add(output_layer)\n",
    "\n",
    "nn3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 45], Loss: 0.6916, Accuracy: 0.5368, Val_loss: 0.6918, Val_accuracy: 0.5101\n",
      "Epoch [10/ 45], Loss: 0.6761, Accuracy: 0.6517, Val_loss: 0.6826, Val_accuracy: 0.6385\n",
      "Epoch [15/ 45], Loss: 0.5628, Accuracy: 0.7642, Val_loss: 0.6455, Val_accuracy: 0.5743\n",
      "Epoch [20/ 45], Loss: 0.4752, Accuracy: 0.7785, Val_loss: 0.8689, Val_accuracy: 0.5236\n",
      "Epoch [25/ 45], Loss: 0.3114, Accuracy: 0.8639, Val_loss: 0.4451, Val_accuracy: 0.7872\n",
      "Epoch [30/ 45], Loss: 0.0555, Accuracy: 0.9966, Val_loss: 0.4547, Val_accuracy: 0.7973\n",
      "Epoch [35/ 45], Loss: 0.0081, Accuracy: 1.0000, Val_loss: 0.5065, Val_accuracy: 0.7973\n",
      "Epoch [40/ 45], Loss: 0.0037, Accuracy: 1.0000, Val_loss: 0.5255, Val_accuracy: 0.7939\n",
      "Epoch [45/ 45], Loss: 0.0023, Accuracy: 1.0000, Val_loss: 0.5512, Val_accuracy: 0.7939\n",
      "Elapsed time: 4.61s\n"
     ]
    }
   ],
   "source": [
    "# Compiling and fitting model configuration 3\n",
    "\n",
    "nn3_model.compile(optimizer=sgd_optimizer,loss=loss_fn,metrics=['accuracy'])\n",
    "t0 = time.time() # start time\n",
    "num_epochs = 45 # epochs\n",
    "history = nn3_model.fit(X_train_tfidf.toarray(),y_train,epochs=num_epochs,verbose=0,validation_split=0.2,callbacks=[ProgBarLoggerNEpochs(num_epochs, every_n=5)])\n",
    "t1 = time.time() # stop time\n",
    "print('Elapsed time: %.2fs' % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5366 - accuracy: 0.7996\n",
      "Loss:  0.5365519523620605 Accuracy:  0.7995951175689697\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Model Configuration 3's performance on test data\n",
    "\n",
    "loss, accuracy = nn3_model.evaluate(X_test_tfidf.toarray(),y_test)\n",
    "print('Loss: ', str(loss) , 'Accuracy: ', str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Selection\n",
    "\n",
    "Out of the 3 model configurations I trained, Model 3 has the least elapsed time at 4.61 s. Model 3 also has the least test loss at approximately 0.54. However, model 1 has the least training loss. Model 1 has the highest test accuracy and both models 1 and 3 reach a training accuracy of 1.000 eventually. I would choose between 1 and 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
